### Taller 4
## Clean the workspace
rm(list=ls())
## Cargar paquetes
require("pacman")
p_load(tidyverse, janitor, tm, stringi, tidytext, stopwords, wordcloud2, udpipe,
ggcorrplot)
test <- read.csv(url("https://github.com/SofiaQuiroga/Repositorio_Taller4_BDML/blob/main/Data/test.csv?raw=true"),encoding = "UTF-8")
train <- read.csv(url("https://github.com/SofiaQuiroga/Repositorio_Taller4_BDML/blob/main/Data/train.csv?raw=true"),encoding = "UTF-8")
## Limpiar variable de texto "tweets"
# Todo en minuscula
test$text <- tolower(test$text)
train$text <- tolower(train$text)
test$text <- stri_trans_general(str = test$text, id = "Latin-ASCII")
train$text <- stri_trans_general(str = train$text, id = "Latin-ASCII")
# Eliminamos caracteres especiales. Remplaza todo lo que no es alfanumericos por un espacio
test$text <- str_replace_all(test$text, "[^[:alnum:]]", " ")
train$text <- str_replace_all(train$text, "[^[:alnum:]]", " ")
# Eliminar números
test$text <- removeNumbers(test$text)
# Eliminar puntuación
test$text <- removePunctuation(test$text)
test$text <- gsub("\\s+", " ", str_trim(test$text))
train$text <- gsub("\\s+", " ", str_trim(train$text))
words_test <- test %>%
unnest_tokens(output = "word", input = "text")
words_train <- train %>%
unnest_tokens(output = "word", input = "text")
## Eliminar stopwords
sw <- c() # vector de todos los stopwords
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
# Unir en el vector sw los stopwords de las diferentes fuentes
sw <- unique(sw) # borra las repeticiones
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII")) # quitar tildes a stopwords
sw <- data.frame(word = sw) # guardar como objeto data frame
# Quitar stopwords
words_test <- words_test %>%
anti_join(sw, by = "word")
words_train <- words_train %>%
anti_join(sw, by = "word")
# Nube de palabras con las 100 palabras más repetidas en la base de entrenamiento
n_words <- words_test %>%
count(word) %>%
arrange(desc(n)) %>%
head(100)
wordcloud2(data = n_words)
#train
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- words_train %>%
distinct(word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>%
select(token, lemma) %>%
rename("word" = "token")
words_train <- words_train %>%
left_join(udpipe_results, by = "word", multiple = "all")
words_train[is.na(words_train$lemma), "lemma"] <- words_train[is.na(words_train$lemma), "word"]
#test
#test
palabras_unicas2 <- words_test %>%
distinct(word)
udpipe_results2 <- udpipe_annotate(model, x = palabras_unicas2$word)
udpipe_results2 <- as_tibble(udpipe_results2)
udpipe_results2 <- udpipe_results2 %>%
select(token, lemma) %>%
rename("word" = "token")
words_test <- words_test %>%
left_join(udpipe_results, by = "word", multiple = "all")
words_test[is.na(words_test$lemma), "lemma"] <- words_test[is.na(words_test$lemma), "word"]
#eliminamos las palabras que menos aparecen
palabras_eliminar <- words_train %>%
count(lemma) %>%
filter(n < 5)
words_train <- words_train %>%
anti_join(palabras_eliminar, by = "lemma")
#Volvemos a nuestro formato original. Comentario por tweets
train_clean <- words_train %>%
group_by(id,name) %>%
summarise(text = str_c(lemma, collapse = " ")) %>%
ungroup()
test_clean <- words_test %>%
group_by(id) %>%
summarise(text = str_c(lemma, collapse = " ")) %>%
ungroup()
# Creamos un corpus
#train
tm_corpus1 <- Corpus(VectorSource(x = train_clean$text))
str(tm_corpus1)
train_tf_idf <- TermDocumentMatrix(tm_corpus1,
control = list(weighting = weightTfIdf))
train_tf_idf <- as.matrix(train_tf_idf) %>%
t() %>%
as.data.frame()
#test
tm_corpus2 <- Corpus(VectorSource(x = test_clean$text))
str(tm_corpus2)
test_tf_idf <- TermDocumentMatrix(tm_corpus2,
control = list(weighting = weightTfIdf))
test_tf_idf <- as.matrix(test_tf_idf) %>%
t() %>%
as.data.frame()
#### Revisamos que todo este ok
train_clean$text[1]
train_tf_idf[1, 1:10]
test_clean$text[1]
test_tf_idf[1, 1:10]
#dismunuimos las columnnas
columnas_seleccionadas <- colSums(train_tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(500) %>%
rownames()
View(test)
View(train)
